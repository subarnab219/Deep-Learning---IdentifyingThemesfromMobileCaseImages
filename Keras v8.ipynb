{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "C:\\Users\\sran12\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "%matplotlib inline \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    height, width, channels = img.shape\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    y_train = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Read train images')\n",
    "    folders = ['Automobile',\n",
    "               'Comics_and_Cartoons',\n",
    "               'Famous_Personalities',\n",
    "               'Festivals_and_Occasions',\n",
    "               'Graffiti_and_Illustrations',\n",
    "               'Movies_and_TV_shows',\n",
    "               'Music',\n",
    "               'Nature',\n",
    "               'No_Theme',\n",
    "               'Patterns_and_Ethnic',\n",
    "               'Signs_and_Symbols',\n",
    "               'Spiritual',\n",
    "               'Sports',\n",
    "               'Superheroes',\n",
    "               'Typography',\n",
    "               'Vintage'\n",
    "              ]\n",
    "    for fld in folders:\n",
    "        index = folders.index(fld)\n",
    "        print('Load folder {} (Index: {})'.format(fld, index))\n",
    "        path = os.path.join('D:\\Misc\\CrowdAnalytix\\Identifying Themes from Mobile Case Images\\Data Raw\\Train', fld, '*.jpg')\n",
    "        files = glob.glob(path)\n",
    "        for i in range(len(files)):\n",
    "            flbase = os.path.basename(files[i])\n",
    "            img = get_im_cv2(files[i])\n",
    "            X_train.append(img)\n",
    "            X_train_id.append(flbase)\n",
    "            y_train.append(index) \n",
    "    print len(X_train)\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return X_train, y_train, X_train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    path = os.path.join('D:\\Misc\\CrowdAnalytix\\Identifying Themes from Mobile Case Images\\Data Raw\\Test', '*.jpg')\n",
    "    files = sorted(glob.glob(path))\n",
    "\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2(fl)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "\n",
    "    return X_test, X_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Load folder Automobile (Index: 0)\n",
      "Load folder Comics_and_Cartoons (Index: 1)\n",
      "Load folder Famous_Personalities (Index: 2)\n",
      "Load folder Festivals_and_Occasions (Index: 3)\n",
      "Load folder Graffiti_and_Illustrations (Index: 4)\n",
      "Load folder Movies_and_TV_shows (Index: 5)\n",
      "Load folder Music (Index: 6)\n",
      "Load folder Nature (Index: 7)\n",
      "Load folder No_Theme (Index: 8)\n",
      "Load folder Patterns_and_Ethnic (Index: 9)\n",
      "Load folder Signs_and_Symbols (Index: 10)\n",
      "Load folder Spiritual (Index: 11)\n",
      "Load folder Sports (Index: 12)\n",
      "Load folder Superheroes (Index: 13)\n",
      "Load folder Typography (Index: 14)\n",
      "Load folder Vintage (Index: 15)\n",
      "4591\n",
      "Read train data time: 99.28 seconds\n"
     ]
    }
   ],
   "source": [
    "Xtrall, Ytrall, Itrall = load_train()\n",
    "X_test, X_test_id = load_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatten out all images to be one-dimensional\n",
    "Xtrall = np.array(Xtrall, dtype=np.uint8)\n",
    "Ytrall = np.array(Ytrall, dtype=np.uint8)\n",
    "\n",
    "Xtrall = Xtrall.astype('float32')\n",
    "Xtrall = Xtrall / 255\n",
    "Xtrall = Xtrall.transpose((0, 3, 1, 2))\n",
    "\n",
    "X_test = np.array(X_test, dtype=np.uint8)\n",
    "\n",
    "X_test = X_test.astype('float32')\n",
    "X_test = X_test / 255\n",
    "X_test = X_test.transpose((0, 3, 1, 2))\n",
    "\n",
    "Ytrall = np_utils.to_categorical(Ytrall, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "nb_classes=len(Ytrall)\n",
    "nb_epoch=40\n",
    "nb_filters=64\n",
    "nb_pool=1\n",
    "nb_conv=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=Xtrall.shape[1:]))\n",
    "model.add(Convolution2D(nb_filters,nb_conv,nb_conv, subsample=(1, 1),activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters,nb_conv,nb_conv, subsample=(1, 1),activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters/2,nb_conv,nb_conv, subsample=(1, 1),activation='relu'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5));\n",
    "model.add(Flatten());\n",
    "model.add(Dense(64));\n",
    "model.add(Dropout(0.5));\n",
    "model.add(Dense(16));\n",
    "model.add(Activation('softmax'));\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start KFold number 1 from 7\n",
      "('Split train: ', 3935, 3935)\n",
      "('Split valid: ', 656, 656)\n",
      "Train on 3935 samples, validate on 656 samples\n",
      "Epoch 1/40\n",
      "173s - loss: 1.9792 - acc: 0.4440 - val_loss: 1.6973 - val_acc: 0.4924\n",
      "Epoch 2/40\n",
      "167s - loss: 1.5350 - acc: 0.5657 - val_loss: 1.4439 - val_acc: 0.5854\n",
      "Epoch 3/40\n",
      "166s - loss: 1.2710 - acc: 0.6379 - val_loss: 1.4116 - val_acc: 0.6052\n",
      "Epoch 4/40\n",
      "160s - loss: 1.0232 - acc: 0.6963 - val_loss: 1.3604 - val_acc: 0.6341\n",
      "Epoch 5/40\n",
      "159s - loss: 0.8054 - acc: 0.7568 - val_loss: 1.3886 - val_acc: 0.6296\n",
      "Epoch 6/40\n",
      "159s - loss: 0.6544 - acc: 0.8030 - val_loss: 1.4377 - val_acc: 0.6296\n",
      "Epoch 7/40\n",
      "177s - loss: 0.5408 - acc: 0.8475 - val_loss: 1.4582 - val_acc: 0.6296\n",
      "Epoch 8/40\n",
      "171s - loss: 0.4496 - acc: 0.8750 - val_loss: 1.5044 - val_acc: 0.6311\n",
      "('Score log_loss: ', 1.5087086835744887)\n",
      "Start KFold number 2 from 7\n",
      "('Split train: ', 3935, 3935)\n",
      "('Split valid: ', 656, 656)\n",
      "Train on 3935 samples, validate on 656 samples\n",
      "Epoch 1/40\n",
      "162s - loss: 0.6107 - acc: 0.8414 - val_loss: 0.2652 - val_acc: 0.9223\n",
      "Epoch 2/40\n",
      "160s - loss: 0.4974 - acc: 0.8607 - val_loss: 0.3094 - val_acc: 0.9024\n",
      "Epoch 3/40\n",
      "161s - loss: 0.4037 - acc: 0.8861 - val_loss: 0.3193 - val_acc: 0.9162\n",
      "Epoch 4/40\n",
      "162s - loss: 0.3619 - acc: 0.8996 - val_loss: 0.3388 - val_acc: 0.9009\n",
      "Epoch 5/40\n",
      "162s - loss: 0.2986 - acc: 0.9189 - val_loss: 0.4053 - val_acc: 0.8826\n",
      "('Score log_loss: ', 0.40533874022955863)\n",
      "Start KFold number 3 from 7\n",
      "('Split train: ', 3935, 3935)\n",
      "('Split valid: ', 656, 656)\n",
      "Train on 3935 samples, validate on 656 samples\n",
      "Epoch 1/40\n",
      "163s - loss: 0.3309 - acc: 0.9095 - val_loss: 0.1359 - val_acc: 0.9543\n",
      "Epoch 2/40\n",
      "162s - loss: 0.2987 - acc: 0.9184 - val_loss: 0.1557 - val_acc: 0.9543\n",
      "Epoch 3/40\n",
      "167s - loss: 0.2803 - acc: 0.9189 - val_loss: 0.1914 - val_acc: 0.9405\n",
      "Epoch 4/40\n",
      "164s - loss: 0.2372 - acc: 0.9291 - val_loss: 0.2128 - val_acc: 0.9405\n",
      "Epoch 5/40\n",
      "173s - loss: 0.2311 - acc: 0.9337 - val_loss: 0.2409 - val_acc: 0.9268\n",
      "('Score log_loss: ', 0.2408836616493025)\n",
      "Start KFold number 4 from 7\n",
      "('Split train: ', 3935, 3935)\n",
      "('Split valid: ', 656, 656)\n",
      "Train on 3935 samples, validate on 656 samples\n",
      "Epoch 1/40\n",
      "181s - loss: 0.2384 - acc: 0.9342 - val_loss: 0.1981 - val_acc: 0.9345\n",
      "Epoch 2/40\n",
      "185s - loss: 0.2210 - acc: 0.9400 - val_loss: 0.2053 - val_acc: 0.9253\n",
      "Epoch 3/40\n",
      "175s - loss: 0.2165 - acc: 0.9441 - val_loss: 0.2556 - val_acc: 0.9192\n",
      "Epoch 4/40\n",
      "184s - loss: 0.1873 - acc: 0.9476 - val_loss: 0.3128 - val_acc: 0.9177\n",
      "Epoch 5/40\n",
      "175s - loss: 0.2045 - acc: 0.9454 - val_loss: 0.3217 - val_acc: 0.9116\n",
      "('Score log_loss: ', 0.32166212188454402)\n",
      "Start KFold number 5 from 7\n",
      "('Split train: ', 3935, 3935)\n",
      "('Split valid: ', 656, 656)\n",
      "Train on 3935 samples, validate on 656 samples\n",
      "Epoch 1/40\n",
      "167s - loss: 0.2402 - acc: 0.9416 - val_loss: 0.1152 - val_acc: 0.9634\n",
      "Epoch 2/40\n",
      "175s - loss: 0.1970 - acc: 0.9469 - val_loss: 0.1624 - val_acc: 0.9558\n",
      "Epoch 3/40\n",
      "179s - loss: 0.1881 - acc: 0.9476 - val_loss: 0.1735 - val_acc: 0.9421\n",
      "Epoch 4/40\n",
      "179s - loss: 0.1760 - acc: 0.9553 - val_loss: 0.2129 - val_acc: 0.9375\n",
      "Epoch 5/40\n",
      "194s - loss: 0.1817 - acc: 0.9520 - val_loss: 0.2592 - val_acc: 0.9329\n",
      "('Score log_loss: ', 0.25922082369143468)\n",
      "Start KFold number 6 from 7\n",
      "('Split train: ', 3935, 3935)\n",
      "('Split valid: ', 656, 656)\n",
      "Train on 3935 samples, validate on 656 samples\n",
      "Epoch 1/40\n",
      "175s - loss: 0.1902 - acc: 0.9489 - val_loss: 0.1105 - val_acc: 0.9634\n",
      "Epoch 2/40\n",
      "171s - loss: 0.1852 - acc: 0.9464 - val_loss: 0.1509 - val_acc: 0.9512\n",
      "Epoch 3/40\n",
      "184s - loss: 0.1790 - acc: 0.9494 - val_loss: 0.2074 - val_acc: 0.9451\n",
      "Epoch 4/40\n",
      "170s - loss: 0.1641 - acc: 0.9520 - val_loss: 0.2189 - val_acc: 0.9421\n",
      "Epoch 5/40\n",
      "173s - loss: 0.1685 - acc: 0.9512 - val_loss: 0.2248 - val_acc: 0.9451\n",
      "('Score log_loss: ', 0.2247547717842889)\n",
      "Start KFold number 7 from 7\n",
      "('Split train: ', 3936, 3936)\n",
      "('Split valid: ', 655, 655)\n",
      "Train on 3936 samples, validate on 655 samples\n",
      "Epoch 1/40\n",
      "176s - loss: 0.1810 - acc: 0.9507 - val_loss: 0.1130 - val_acc: 0.9527\n",
      "Epoch 2/40\n",
      "185s - loss: 0.1707 - acc: 0.9568 - val_loss: 0.1516 - val_acc: 0.9420\n",
      "Epoch 3/40\n",
      "170s - loss: 0.1710 - acc: 0.9540 - val_loss: 0.1846 - val_acc: 0.9389\n",
      "Epoch 4/40\n",
      "185s - loss: 0.1505 - acc: 0.9578 - val_loss: 0.2100 - val_acc: 0.9389\n",
      "Epoch 5/40\n",
      "180s - loss: 0.1511 - acc: 0.9566 - val_loss: 0.2446 - val_acc: 0.9359\n",
      "('Score log_loss: ', 0.24458633585987746)\n"
     ]
    }
   ],
   "source": [
    "nfolds = 7\n",
    "yfull_train = dict()\n",
    "kf = KFold(len(Itrall), n_folds=nfolds, shuffle=True, random_state=51)\n",
    "num_fold = 0\n",
    "sum_score = 0\n",
    "models = []\n",
    "\n",
    "for mtr,mval in kf:\n",
    "    Xtr = Xtrall[mtr]\n",
    "    Ytr = Ytrall[mtr]\n",
    "    Xval = Xtrall[mval]\n",
    "    Yval = Ytrall[mval]\n",
    "    \n",
    "    num_fold += 1\n",
    "    \n",
    "    print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "    print('Split train: ', len(Xtr), len(Ytr))\n",
    "    print('Split valid: ', len(Xval), len(Yval))\n",
    "    \n",
    "    callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n",
    "        ]\n",
    "    model.fit(Xtr,Ytr, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              shuffle=True, verbose=2, validation_data=(Xval, Yval),\n",
    "              callbacks=callbacks)\n",
    "\n",
    "    predictions_valid = model.predict(Xval.astype('float32'), batch_size=batch_size, verbose=2)\n",
    "    score = log_loss(Yval, predictions_valid)\n",
    "    print('Score log_loss: ', score)\n",
    "    sum_score += score*len(mval)\n",
    "    \n",
    "    # Store valid predictions\n",
    "    for i in range(len(mval)):\n",
    "        yfull_train[mval[i]] = predictions_valid[i]\n",
    "\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Log_loss train independent avg: ', 0.45792576445958461)\n"
     ]
    }
   ],
   "source": [
    "score = sum_score/len(Xtrall)\n",
    "print(\"Log_loss train independent avg: \", score)\n",
    "\n",
    "info_string = 'loss_' + str(score) + '_folds_' + str(10) + '_ep_' + str(nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start KFold number 1 from 7\n",
      "Start KFold number 2 from 7\n",
      "Start KFold number 3 from 7\n",
      "Start KFold number 4 from 7\n",
      "Start KFold number 5 from 7\n",
      "Start KFold number 6 from 7\n",
      "Start KFold number 7 from 7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_fold = 0\n",
    "yfull_test = []\n",
    "test_id = []\n",
    "nfolds = len(models)\n",
    "\n",
    "for i in range(nfolds):\n",
    "    model = models[i]\n",
    "    num_fold += 1\n",
    "    print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "    test_prediction = model.predict(X_test, batch_size=batch_size, verbose=2)\n",
    "    yfull_test.append(test_prediction)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array(yfull_test[0])\n",
    "for i in range(1, nfolds):\n",
    "    a += np.array(yfull_test[i])\n",
    "a /= nfolds\n",
    "test_res = a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_string = 'loss_' + info_string \\\n",
    "                + '_folds_' + str(nfolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"0\":\"Automobile\",\"1\":\"Comics_and_Cartoons\",\"2\":\"Famous_Personalities\",\"3\":\"Festivals_and_Occasions\",\n",
    "              \"4\":\"Graffiti_and_Illustrations\",\"5\":\"Movies_and_TV_shows\",\"6\":\"Music\",\"7\":\"Nature\",\"8\":\"No_Theme\",\n",
    "              \"9\":\"Patterns_and_Ethnic\",\"10\":\"Signs_and_Symbols\",\"11\":\"Spiritual\",\"12\":\"Sports\",\"13\":\"Superheroes\",\n",
    "              \"14\":\"Typography\",\"15\":\"Vintage\"}\n",
    "    \n",
    "y = np.array([params[str(i)] for i in np.argmax(test_res, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = pd.DataFrame(X_test_id, columns=['id'])\n",
    "result1.loc[:, 'Mobile_Theme'] = pd.Series(y, index=result1.index)\n",
    "now = datetime.datetime.now()\n",
    "sub_file = 'submission_keras_v4' +'_'+info_string+  '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
